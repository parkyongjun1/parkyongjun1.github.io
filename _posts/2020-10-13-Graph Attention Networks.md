---
title: Paper Review. Graph Attention Networks@ICLR' 2018
author: YongJun Park
date: 2020-10-13 18:00:00 +0900
categories: [Paper Reviews, ML]
tags: [Attention, Graph]
math: true
pin: True
---

- [Paper link]()


## **Abstract**
<img src="/assets/papers/Graph Attention Networks/(4).png" width='800'>

## **Attention**
<img src='/assets/papers/Graph Attention Networks/(6).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(7).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(8).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(9).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(10).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(11).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(12).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(13).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(14).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(15).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(16).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(17).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(18).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(19).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(20).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(21).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(22).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(23).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(24).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(25).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(26).png' width='800'>

## **Graph Neural Networks**
<img src='/assets/papers/Graph Attention Networks/(28).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(29).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(30).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(31).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(32).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(33).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(34).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(35).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(36).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(37).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(38).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(39).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(40).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(41).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(42).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(43).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(44).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(45).png' width='800'>

## **Graph Attention Networks**
<img src='/assets/papers/Graph Attention Networks/(47).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(48).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(49).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(50).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(51).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(52).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(53).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(54).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(55).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(56).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(57).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(58).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(59).png' width='800'>

## **Experiments**
<img src='/assets/papers/Graph Attention Networks/(60).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(61).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(62).png' width='800'>
<img src='/assets/papers/Graph Attention Networks/(63).png' width='800'>



## **Conclusions & Reviews**
- Attention 을 이용해 RNN, CNN 의 구조의 단점을 극복한 점이 흥미로웠다.

- Text 나 Image 데이터를 그래프로 표현해 처리하는 방법에 대해 배울 수 있었다.

- 본 논문을 이해하기위해 필요한 Attention 과 GNN 에 대해 공부할 수 있었다.

- 기본 GNN 에 Attention 을 적용해 성능을 올린 점이 흥미로웠다.

- 논문 자체는 단순히 Attention 과 GNN 을 함께 쓰는 것인데 두 가지 개념을 확실히 이해하지 못해 순서대로 개념을 이해하는데 시간이 조금 걸렸다.


## **Reference**
- 고려대학교 세미나 자료 : https://www.youtube.com/watch?v=NSjpECvEf0Y
