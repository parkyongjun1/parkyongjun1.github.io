---
title: Paper Review. An all-MLP Architecture for Vision@arXiv’2021
author: YongJun Park
date: 2021-08-18 18:00:00 +0900
categories: [Paper Reviews, CV]
tags: [Image Classification, Transformer, MLP]
math: true
pin: True
---

- [Paper link](https://arxiv.org/abs/2105.01601)


## **Abstract**
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/11.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/12.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/13.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/14.png" width='800'>

## **Introduction**
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/3.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/4.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/5.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/6.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/7.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/8.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/9.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/10.png" width='800'>

## **MLP-Mixer**
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/15.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/16.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/17.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/18.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/19.png" width='800'>

## **Experiments**
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/20.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/21.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/22.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/23.png" width='800'>
<img src="/assets/papers/MLP_Mixer_An all_MLP Architecture for Vision/24.png" width='800'>


## **Conclusions & Reviews**
- CNN과 Self-attention을 사용하지 않는 MLP-Mixer를 제안함.

- 이미지 처리에 대한 2가지(Cross location, Per location) 중요점을 명시적으로 제안함.

- 처리량, 성능, 시간 대비 기존 SOTA 모델에 준하는 성능을 달성함.

- 근본적으로 돌아와서 MLP로 성능을 개선하고 처리량 등을 개선한 점이 놀라웠다.

- inductive bias의 이론적 이해와 직관이 CV 연구에 어쩌면 가장 중요한 점이 아닐까라는 생각을 가졌다.


## **Reference**
