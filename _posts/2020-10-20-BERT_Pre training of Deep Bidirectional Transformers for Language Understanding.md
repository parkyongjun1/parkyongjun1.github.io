---
title: Paper Review. BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding@NAACL' 2019
author: YongJun Park
date: 2020-10-20 18:00:00 +0900
categories: [Paper Reviews, NLP]
tags: [Pretrain, Transformer]
math: true
pin: True
---

- [Paper link](https://arxiv.org/abs/1810.04805)


## **Abstract**
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/4.png" width='800'>

## **Background**
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/6.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/7.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/8.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/9.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/10.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/11.png" width='800'>

## **BERT**
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/13.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/14.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/15.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/16.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/17.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/18.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/19.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/20.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/21.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/22.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/23.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/24.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/25.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/26.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/27.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/28.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/29.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/30.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/31.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/32.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/33.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/34.png" width='800'>

## **Experiments**
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/36.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/37.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/38.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/39.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/40.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/41.png" width='800'>
<img src="/assets/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/42.png" width='800'>



## **Conclusions & Reviews**
- bidirectional 구조를 일반화시켜 Pre training 된 모델이 광범위한 NLP 작업을 성공적으로 처리할 수 있도록 기여함.

- Fine tuning 을 위해 단순하게 bert 의 마지막 단에 한 개의 레이어를 추가해서 downstream task 를 수행한다는 점이 놀라웠다.

- 모델을 복잡하게 만들어서 성능을 올릴 수도 있지만 반대로 단순하게 접근해서 강력한 성능을 낼 수 있었다는 점이 매우 흥미로웠다.


## **Reference**

